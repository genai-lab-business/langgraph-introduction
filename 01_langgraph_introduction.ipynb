{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mDXHmxnxnTF"
      },
      "source": [
        "# LangGraph Introduction: Building Intelligent Workflows with OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nH7ciTbcxwHN"
      },
      "source": [
        "This notebook demonstrates the fundamentals of LangGraph, a powerful library for creating stateful, multi-step workflows with Large Language Models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrmrrE8fxxse"
      },
      "source": [
        "## What You'll Learn\n",
        "\n",
        "1. Basic LangGraph concepts and setup  \n",
        "2. Creating simple linear workflows  \n",
        "3. Building conditional flows  \n",
        "4. State management across nodes  \n",
        "5. Integration with OpenAI's GPT models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNr3UyRpx1bj"
      },
      "source": [
        "## Setup and Installation\n",
        "\n",
        "First, let's install the required packages and set up our environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P556uzgLx7-j"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install langgraph langchain-openai python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiI2BdgfxrFM",
        "outputId": "1a7ed39f-3a69-449c-c783-230d3af98836"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from typing import Dict, Any, List, Annotated\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "# LangGraph imports\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "# LangChain imports\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.tools import tool\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "print(\"✅ Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWI-580DyCJ1"
      },
      "source": [
        "## Configure OpenAI API\n",
        "\n",
        "Make sure you have your OpenAI API key set up in your `.env` file or as an environment variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UF2kR392yC61",
        "outputId": "a6a25d30-fd9f-4950-b25d-efaa051130e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ OpenAI connection successful: Hello, LangGraph!\n"
          ]
        }
      ],
      "source": [
        "# Initialize OpenAI model\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0.1,\n",
        "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
        ")\n",
        "\n",
        "# Test the connection\n",
        "try:\n",
        "    test_response = llm.invoke(\"Say 'Hello, LangGraph!'\")\n",
        "    print(f\"✅ OpenAI connection successful: {test_response.content}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ OpenAI connection failed: {e}\")\n",
        "    print(\"Please check your API key in the .env file\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwwpnhxbyLmB"
      },
      "source": [
        "## 1. Basic LangGraph Concepts\n",
        "\n",
        "Let's start with the fundamental building blocks of LangGraph.\n",
        "\n",
        "### State Definition\n",
        "\n",
        "In LangGraph, state is passed between nodes. We define it using `TypedDict`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoZB7136yMEU",
        "outputId": "e1cdd6ac-8729-4ae7-c277-6eaa84b45421"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ State definition created!\n"
          ]
        }
      ],
      "source": [
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "    current_step: str\n",
        "    user_input: str\n",
        "    analysis_result: str\n",
        "    summary: str\n",
        "    final_response: str\n",
        "\n",
        "print(\"✅ State definition created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYeT9ecXyUFd"
      },
      "source": [
        "### Node Functions\n",
        "\n",
        "Each node in the graph is a function that takes the state and returns an updated state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oq-eLgvEyUoL",
        "outputId": "c9407f90-f9af-4cee-a11b-e7154a93f3f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Node functions defined!\n"
          ]
        }
      ],
      "source": [
        "def analyze_input(state: AgentState) -> AgentState:\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        \"Analyze the following text and identify the main topic, sentiment, and key points:\\n\\n{text}\\n\\nProvide a structured analysis.\"\n",
        "    )\n",
        "    chain = prompt | llm\n",
        "    result = chain.invoke({\"text\": state[\"user_input\"]})\n",
        "    return {\n",
        "        \"analysis_result\": result.content,\n",
        "        \"current_step\": \"analysis_complete\",\n",
        "        \"messages\": [result]\n",
        "    }\n",
        "\n",
        "def summarize_analysis(state: AgentState) -> AgentState:\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        \"Create a concise summary of the following analysis:\\n\\n{analysis}\\n\\nSummary:\"\n",
        "    )\n",
        "    chain = prompt | llm\n",
        "    result = chain.invoke({\"analysis\": state[\"analysis_result\"]})\n",
        "    return {\n",
        "        \"summary\": result.content,\n",
        "        \"current_step\": \"summary_complete\",\n",
        "        \"messages\": [result]\n",
        "    }\n",
        "\n",
        "def generate_response(state: AgentState) -> AgentState:\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        \"Based on the analysis and summary below, generate a helpful response to the user:\\n\\nAnalysis: {analysis}\\n\\nSummary: {summary}\\n\\nOriginal Input: {input}\\n\\nResponse:\"\n",
        "    )\n",
        "    chain = prompt | llm\n",
        "    result = chain.invoke({\n",
        "        \"analysis\": state[\"analysis_result\"],\n",
        "        \"summary\": state[\"summary\"],\n",
        "        \"input\": state[\"user_input\"]\n",
        "    })\n",
        "    return {\n",
        "        \"final_response\": result.content,\n",
        "        \"current_step\": \"complete\",\n",
        "        \"messages\": [result]\n",
        "    }\n",
        "\n",
        "print(\"✅ Node functions defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHiIFx9A1sgv"
      },
      "source": [
        "## 2. Creating Your First LangGraph Workflow\n",
        "\n",
        "Now let's create a simple linear workflow that processes input through three steps:\n",
        "1. Analyze input\n",
        "2. Summarize the analysis\n",
        "3. Generate a final response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdGBXxvNzUtM",
        "outputId": "1623015b-dc86-4f9f-bc72-e019037f539d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Basic workflow compiled!\n"
          ]
        }
      ],
      "source": [
        "def create_basic_workflow():\n",
        "    \"\"\"Create a basic linear LangGraph workflow\"\"\"\n",
        "\n",
        "    workflow = StateGraph(AgentState)\n",
        "\n",
        "    # Add nodes\n",
        "    workflow.add_node(\"analyze\", analyze_input)\n",
        "    workflow.add_node(\"summarize\", summarize_analysis)\n",
        "    workflow.add_node(\"respond\", generate_response)\n",
        "\n",
        "    # Add edges (defines flow)\n",
        "    workflow.add_edge(START, \"analyze\")\n",
        "    workflow.add_edge(\"analyze\", \"summarize\")\n",
        "    workflow.add_edge(\"summarize\", \"respond\")\n",
        "    workflow.add_edge(\"respond\", END)\n",
        "\n",
        "    return workflow.compile()\n",
        "\n",
        "basic_app = create_basic_workflow()\n",
        "print(\"✅ Basic workflow compiled!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXqdTAex2Psk"
      },
      "source": [
        "## 3. Running the Workflow\n",
        "\n",
        "Let's test our linear workflow with a sample user input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFDeqrBI2M17",
        "outputId": "cb7b2679-cd52-446e-9d46-db7f564c2302"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 Running workflow...\n",
            "\n",
            "==================================================\n",
            "📊 ANALYSIS RESULT\n",
            "==================================================\n",
            "### Analysis of the Text\n",
            "\n",
            "**Main Topic:**\n",
            "- The text discusses the author's enthusiasm for learning about LangGraph, a tool for building AI workflows.\n",
            "\n",
            "**Sentiment:**\n",
            "- The sentiment is positive. The author expresses excitement and optimism about the potential of LangGraph.\n",
            "\n",
            "**Key Points:**\n",
            "1. **Excitement for Learning:** The author is eager to learn about LangGraph.\n",
            "2. **Powerful Tool:** LangGraph is described as a powerful tool, indicating its capabilities and effectiveness.\n",
            "3. **Applications in Work:** The author sees many potential applications for LangGraph in their professional context, suggesting its relevance and utility in their field.\n",
            "\n",
            "==================================================\n",
            "📝 SUMMARY\n",
            "==================================================\n",
            "The text conveys the author's positive sentiment and enthusiasm for learning about LangGraph, a powerful tool for building AI workflows. The author expresses eagerness to explore its capabilities and recognizes its potential applications in their professional context, highlighting its relevance and utility in their field.\n",
            "\n",
            "==================================================\n",
            "💬 FINAL RESPONSE\n",
            "==================================================\n",
            "It's great to hear about your excitement for learning LangGraph! It truly is a powerful tool for building AI workflows, and your enthusiasm suggests you're ready to explore its capabilities. As you dive into it, consider how it can streamline your processes and enhance your projects. If you have any specific applications in mind or questions as you learn, feel free to share! I'm here to help you along the way.\n",
            "\n",
            "✅ Workflow completed successfully!\n"
          ]
        }
      ],
      "source": [
        "user_input = \"I'm really excited about learning LangGraph! It seems like a powerful tool for building complex AI workflows. I can see many potential applications in my work.\"\n",
        "\n",
        "print(\"🚀 Running workflow...\\n\")\n",
        "\n",
        "try:\n",
        "    result = basic_app.invoke({\"user_input\": user_input})\n",
        "\n",
        "    print(\"=\" * 50)\n",
        "    print(\"📊 ANALYSIS RESULT\")\n",
        "    print(\"=\" * 50)\n",
        "    print(result[\"analysis_result\"])\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"📝 SUMMARY\")\n",
        "    print(\"=\" * 50)\n",
        "    print(result[\"summary\"])\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"💬 FINAL RESPONSE\")\n",
        "    print(\"=\" * 50)\n",
        "    print(result[\"final_response\"])\n",
        "\n",
        "    print(\"\\n✅ Workflow completed successfully!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error running workflow: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVlEai7c3Iwo"
      },
      "source": [
        "## 4. Building Conditional Workflows\n",
        "\n",
        "LangGraph supports conditional logic to dynamically route the workflow based on decisions made at runtime.\n",
        "\n",
        "In this section, we’ll:\n",
        "- Classify the input (technical / creative / general)\n",
        "- Route to the appropriate handler node"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxvY5xPN3QwE"
      },
      "source": [
        "🧾 State Update"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "WPj1Zt632mOW"
      },
      "outputs": [],
      "source": [
        "class ConditionalAgentState(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "    current_step: str\n",
        "    user_input: str\n",
        "    classification: str\n",
        "    final_response: str"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeeiZVN43RyO"
      },
      "source": [
        "🔎 Classification Node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "80EEBWzE3MCG"
      },
      "outputs": [],
      "source": [
        "def classify_input(state: ConditionalAgentState) -> ConditionalAgentState:\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        \"Classify the following text as either 'technical', 'creative', or 'general':\\n\\n{text}\\n\\nClassification (respond with just one word):\"\n",
        "    )\n",
        "    chain = prompt | llm\n",
        "    result = chain.invoke({\"text\": state[\"user_input\"]})\n",
        "    return {\n",
        "        \"classification\": result.content.strip().lower(),\n",
        "        \"current_step\": \"classified\",\n",
        "        \"messages\": [result]\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1G0E57c3b0c"
      },
      "source": [
        "🧠 Handler Nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "tGDlbGqU3YH3"
      },
      "outputs": [],
      "source": [
        "def technical_handler(state: ConditionalAgentState) -> ConditionalAgentState:\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        \"Provide a technical analysis of the following text, including implementation details and best practices:\\n\\n{text}\"\n",
        "    )\n",
        "    chain = prompt | llm\n",
        "    result = chain.invoke({\"text\": state[\"user_input\"]})\n",
        "    return {\n",
        "        \"final_response\": result.content,\n",
        "        \"current_step\": \"technical_complete\",\n",
        "        \"messages\": [result]\n",
        "    }\n",
        "\n",
        "def creative_handler(state: ConditionalAgentState) -> ConditionalAgentState:\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        \"Provide a creative interpretation of the following text, focusing on mood, theme, and literary style:\\n\\n{text}\"\n",
        "    )\n",
        "    chain = prompt | llm\n",
        "    result = chain.invoke({\"text\": state[\"user_input\"]})\n",
        "    return {\n",
        "        \"final_response\": result.content,\n",
        "        \"current_step\": \"creative_complete\",\n",
        "        \"messages\": [result]\n",
        "    }\n",
        "\n",
        "def general_handler(state: ConditionalAgentState) -> ConditionalAgentState:\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        \"Provide a general overview of the following text, summarizing the main ideas and relevance:\\n\\n{text}\"\n",
        "    )\n",
        "    chain = prompt | llm\n",
        "    result = chain.invoke({\"text\": state[\"user_input\"]})\n",
        "    return {\n",
        "        \"final_response\": result.content,\n",
        "        \"current_step\": \"general_complete\",\n",
        "        \"messages\": [result]\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvX5Kak63kDw"
      },
      "source": [
        "🧭 Decision Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "KI3K7aow3gyI"
      },
      "outputs": [],
      "source": [
        "def decide_path(state: ConditionalAgentState) -> str:\n",
        "    classification = state.get(\"classification\", \"\").lower()\n",
        "    if \"technical\" in classification:\n",
        "        return \"technical\"\n",
        "    elif \"creative\" in classification:\n",
        "        return \"creative\"\n",
        "    return \"general\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SU-MJ8DP3rjd"
      },
      "source": [
        "🧩 Create Conditional Workflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3o8qpYTB3o34",
        "outputId": "5ee91912-ea74-46a6-d713-66ea8e6b39d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Conditional workflow created!\n"
          ]
        }
      ],
      "source": [
        "def create_conditional_workflow():\n",
        "    workflow = StateGraph(ConditionalAgentState)\n",
        "\n",
        "    workflow.add_node(\"classify\", classify_input)\n",
        "    workflow.add_node(\"technical\", technical_handler)\n",
        "    workflow.add_node(\"creative\", creative_handler)\n",
        "    workflow.add_node(\"general\", general_handler)\n",
        "\n",
        "    workflow.add_conditional_edges(\n",
        "        \"classify\",\n",
        "        decide_path,\n",
        "        {\n",
        "            \"technical\": \"technical\",\n",
        "            \"creative\": \"creative\",\n",
        "            \"general\": \"general\"\n",
        "        }\n",
        "    )\n",
        "\n",
        "    workflow.add_edge(START, \"classify\")\n",
        "    workflow.add_edge(\"technical\", END)\n",
        "    workflow.add_edge(\"creative\", END)\n",
        "    workflow.add_edge(\"general\", END)\n",
        "\n",
        "    return workflow.compile()\n",
        "\n",
        "conditional_app = create_conditional_workflow()\n",
        "print(\"✅ Conditional workflow created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZtgCnWY39MD"
      },
      "source": [
        "## 5. Testing the Conditional Workflow\n",
        "\n",
        "We’ll now test our conditional LangGraph pipeline with different types of input:\n",
        "- Technical\n",
        "- Creative\n",
        "- General\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Neakbr6F4vea"
      },
      "source": [
        "🧪 Test: Technical Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHPi9xiB30Lc",
        "outputId": "248a86a3-3ef6-4450-9a73-70a76556e2cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔧 Testing TECHNICAL input...\n",
            "\n",
            "📊 Classification: technical\n",
            "🎯 Final Step: technical_complete\n",
            "\n",
            "💬 Final Response:\n",
            "\n",
            "Building a REST API using FastAPI and PostgreSQL involves several considerations, especially when it comes to authentication and background task processing. Below is a technical analysis of how to implement these features efficiently, along with best practices.\n",
            "\n",
            "### 1. Authentication\n",
            "\n",
            "#### Implementation Details\n",
            "\n",
            "FastAPI provides several ways to handle authentication, with OAuth2 being one of the most common methods. Here’s a step-by-step guide to implementing OAuth2 with password flow:\n",
            "\n",
            "1. **Install Required Packages**:\n",
            "   ```bash\n",
            "   pip install fastapi[all] psycopg2-binary python-jose[cryptography] passlib[bcrypt]\n",
            "   ```\n",
            "\n",
            "2. **Database Setup**:\n",
            "   Ensure you have a users table in your PostgreSQL database with fields for username, hashed password, and any other relevant user information.\n",
            "\n",
            "3. **Password Hashing**:\n",
            "   Use `passlib` to hash passwords before storing them in the database.\n",
            "   ```python\n",
            "   from passlib.context import CryptContext\n",
            "\n",
            "   pwd_context = CryptContext(schemes=[\"bcrypt\"], deprecated=\"auto\")\n",
            "\n",
            "   def hash_password(password: str) -> str:\n",
            "       return pwd_context.hash(password)\n",
            "\n",
            "   def verify_password(plain_password: str, hashed_password: str) -> bool:\n",
            "       return pwd_context.verify(plain_password, hashed_password)\n",
            "   ```\n",
            "\n",
            "4. **Token Creation**:\n",
            "   Use `python-jose` to create and verify JWT tokens.\n",
            "   ```python\n",
            "   from datetime import datetime, timedelta\n",
            "   from jose import JWTError, jwt\n",
            "\n",
            "   SECRET_KEY = \"your_secret_key\"\n",
            "   ALGORITHM = \"HS256\"\n",
            "   ACCESS_TOKEN_EXPIRE_MINUTES = 30\n",
            "\n",
            "   def create_access_token(data: dict, expires_delta: timedelta = None):\n",
            "       to_encode = data.copy()\n",
            "       if expires_delta:\n",
            "           expire = datetime.utcnow() + expires_delta\n",
            "       else:\n",
            "           expire = datetime.utcnow() + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)\n",
            "       to_encode.update({\"exp\": expire})\n",
            "       encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)\n",
            "       return encoded_jwt\n",
            "   ```\n",
            "\n",
            "5. **Dependency Injection**:\n",
            "   Create a dependency that checks for valid tokens.\n",
            "   ```python\n",
            "   from fastapi import Depends, HTTPException, status\n",
            "   from fastapi.security import OAuth2PasswordBearer\n",
            "\n",
            "   oauth2_scheme = OAuth2PasswordBearer(tokenUrl=\"token\")\n",
            "\n",
            "   async def get_current_user(token: str = Depends(oauth2_scheme)):\n",
            "       credentials_exception = HTTPException(\n",
            "           status_code=status.HTTP_401_UNAUTHORIZED,\n",
            "           detail=\"Could not validate credentials\",\n",
            "           headers={\"WWW-Authenticate\": \"Bearer\"},\n",
            "       )\n",
            "       try:\n",
            "           payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])\n",
            "           username: str = payload.get(\"sub\")\n",
            "           if username is None:\n",
            "               raise credentials_exception\n",
            "           # Fetch user from database\n",
            "       except JWTError:\n",
            "           raise credentials_exception\n",
            "   ```\n",
            "\n",
            "#### Best Practices\n",
            "\n",
            "- **Use HTTPS**: Always serve your API over HTTPS to protect sensitive data.\n",
            "- **Token Expiry**: Implement short-lived access tokens and refresh tokens to enhance security.\n",
            "- **Rate Limiting**: Consider implementing rate limiting to prevent abuse of your API.\n",
            "- **Logging**: Log authentication attempts and failures for monitoring and auditing purposes.\n",
            "\n",
            "### 2. Background Task Processing\n",
            "\n",
            "FastAPI provides a simple way to handle background tasks using the `BackgroundTasks` class. Here’s how to implement it:\n",
            "\n",
            "#### Implementation Details\n",
            "\n",
            "1. **Define Background Tasks**:\n",
            "   Create a function that will run in the background.\n",
            "   ```python\n",
            "   from fastapi import BackgroundTasks\n",
            "\n",
            "   def send_email(email: str, message: str):\n",
            "       # Logic to send email\n",
            "       pass\n",
            "   ```\n",
            "\n",
            "2. **Use BackgroundTasks in Endpoints**:\n",
            "   Inject `BackgroundTasks` into your endpoint to run tasks after returning a response.\n",
            "   ```python\n",
            "   from fastapi import FastAPI\n",
            "\n",
            "   app = FastAPI()\n",
            "\n",
            "   @app.post(\"/send-email/\")\n",
            "   async def send_email_endpoint(email: str, message: str, background_tasks: BackgroundTasks):\n",
            "       background_tasks.add_task(send_email, email, message)\n",
            "       return {\"message\": \"Email will be sent in the background\"}\n",
            "   ```\n",
            "\n",
            "#### Best Practices\n",
            "\n",
            "- **Task Queue**: For more complex background processing, consider using a task queue like Celery or RQ. This allows for better scalability and reliability.\n",
            "- **Error Handling**: Implement error handling in your background tasks to ensure failures are logged and managed appropriately.\n",
            "- **Monitoring**: Use monitoring tools to track the performance and success of background tasks.\n",
            "- **Resource Management**: Be mindful of resource usage, especially if tasks are CPU or memory-intensive.\n",
            "\n",
            "### Conclusion\n",
            "\n",
            "By following the outlined implementation details and best practices, you can efficiently handle authentication and background task processing in your FastAPI application with PostgreSQL. Always keep security, scalability, and maintainability in mind as you develop your API.\n"
          ]
        }
      ],
      "source": [
        "user_input = \"I'm building a REST API using FastAPI and PostgreSQL. How should I handle authentication and background task processing efficiently?\"\n",
        "\n",
        "print(\"🔧 Testing TECHNICAL input...\\n\")\n",
        "\n",
        "try:\n",
        "    result = conditional_app.invoke({\"user_input\": user_input})\n",
        "    print(f\"📊 Classification: {result['classification']}\")\n",
        "    print(f\"🎯 Final Step: {result['current_step']}\")\n",
        "    print(\"\\n💬 Final Response:\\n\")\n",
        "    print(result[\"final_response\"])\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXUoDz_D4gKf"
      },
      "source": [
        "🎨 Test: Creative Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziv4astf4Lew",
        "outputId": "74372f1d-aa91-448a-8092-5c149d58286f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🎨 Testing CREATIVE input...\n",
            "\n",
            "📊 Classification: creative\n",
            "🎯 Final Step: creative_complete\n",
            "\n",
            "💬 Final Response:\n",
            "\n",
            "In the hushed embrace of twilight, the wind became a soft-spoken confidant, weaving its way through the gnarled branches of ancient trees. Each rustle of leaves carried with it the weight of untold stories, secrets that danced on the edge of memory, elusive yet tantalizing. The moon, a silent sentinel in the night sky, draped the landscape in a silvery veil, its light spilling like liquid nostalgia across the field. Shadows stretched and curled, reminiscent of dreams long abandoned, flickering at the periphery of consciousness.\n",
            "\n",
            "The atmosphere was thick with a sense of yearning, a bittersweet nostalgia that hung in the air like the scent of rain on dry earth. The trees stood as guardians of these whispered tales, their bark etched with the passage of time, while the moonlight painted the world in shades of longing and reflection. Each shadow seemed to pulse with life, a reminder of what once was and what might have been, inviting the heart to wander through the corridors of its own forgotten aspirations.\n",
            "\n",
            "In this enchanted moment, the night became a canvas, and the wind, the brush that swept across it, creating a tapestry of emotions that resonated deep within the soul. The field, bathed in ethereal light, transformed into a sanctuary for dreams, where the past and present intertwined, and the echoes of the night beckoned the weary spirit to listen closely, to embrace the mysteries that lingered just beyond the veil of the ordinary.\n"
          ]
        }
      ],
      "source": [
        "user_input = \"The wind whispered secrets through the trees as the moon cast shadows like forgotten dreams across the field.\"\n",
        "\n",
        "print(\"\\n🎨 Testing CREATIVE input...\\n\")\n",
        "\n",
        "try:\n",
        "    result = conditional_app.invoke({\"user_input\": user_input})\n",
        "    print(f\"📊 Classification: {result['classification']}\")\n",
        "    print(f\"🎯 Final Step: {result['current_step']}\")\n",
        "    print(\"\\n💬 Final Response:\\n\")\n",
        "    print(result[\"final_response\"])\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzOlM5Ja44AR"
      },
      "source": [
        "## 6. Conclusion and Next Steps\n",
        "\n",
        "Congratulations! You've built both linear and conditional LangGraph workflows from scratch using OpenAI’s GPT models.\n",
        "\n",
        "---\n",
        "\n",
        "### What you learned:\n",
        "\n",
        "- How to define and track state with `TypedDict`\n",
        "- How to implement clean node functions that return only updated fields\n",
        "- How to route execution flow using linear or conditional logic\n",
        "- How to build production-ready LLM pipelines with LangGraph\n",
        "\n",
        "---\n",
        "\n",
        "### Useful Resources:\n",
        "\n",
        "- 🔗 [LangGraph Docs](https://langchain-ai.github.io/langgraph/)\n",
        "- 🔗 [LangChain Docs](https://docs.langchain.com/)\n",
        "- 🔗 [OpenAI Python SDK](https://github.com/openai/openai-python)\n",
        "\n",
        "---\n",
        "\n",
        "### Read the full article on Medium:**\n",
        "[LangGraph Introduction: Building Intelligent Workflows with OpenAI](https://medium.com/@your-medium-handle/langgraph-introduction-building-intelligent-workflows-with-openai-xxxxx)\n",
        "\n",
        "🤝 **Thanks for following along — happy building with LangGraph!**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "LWI-580DyCJ1",
        "UwwpnhxbyLmB"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
